id;series;booktitle;year;title;abstract;authors;pdf1page;pdfarticle;MS;place;Latitude;Longitude
1188;Revue des Nouvelles Technologies de l'Information;EGC;2017;A Hybrid Approach for Detecting Influencers in Social Media;La détection d'influenceurs dans les réseaux sociaux s'appuie généra-lement sur une structure de graphe représentant les utilisateurs et leurs interac-tions. Récemment, cette tâche a tenu compte, en sus de la structure du graphe,du contenu textuel généré par les utilisateurs. Notre approche s'inscrit dans cettelignée : des informations sont extraites du contenu textuel par des règles linguis-tiques puis sont intégrées dans un système d'apprentissage automatique. Nousmontrerons le prototype développé et son interface de visualisation qui facilitel'interprétation des résultats.;Ioannis Partalas, Cédric Lopez, Pierre-Alain Avouac, Matthieu Osmuk, Domoina Rabarijaona, Dana Popovici, Frédérique Segond;http://editions-rnti.fr/render_pdf.php?p1&p=1002325;http://editions-rnti.fr/render_pdf.php?p=1002325;11;Grenoble;45.188529;5.724524
1189;Revue des Nouvelles Technologies de l'Information;EGC;2017;Analyse des dynamiques spatio-temporelles à partir de séries temporelles d'images satellitaires;La télédétection est un domaine qui regroupe les techniques et lesoutils permettant l'observation de la terre, notamment l'acquisition d'images sa-tellitaires. La méthode proposée dans cet article permet une analyse automatiquede séries temporelles de telles images. Nos travaux introduisent un nouvelle ap-proche pour l'analyse et le clustering de Séries Temporelles d'Images Satelli-taire (STIS). Ce processus se divise en deux parties. Dans un premier temps,nous retraçons les changements radiométriques d'une zone en représentant sonévolution au cours du temps par un graphe dit graphe d'évolution. Dans undeuxième temps, nous introduisons une représentation synthétique des graphesd'évolutions afin de pouvoir appliquer un algorithme de clustering permettantun regroupement par types d'évolutions identifiées. Les expérimentations me-nées nous ont permis de valider notre approche sur une zone d'étude.;Lynda Khiali, Dino Ienco, Maguelonne Teisseire;http://editions-rnti.fr/render_pdf.php?p1&p=1002286;http://editions-rnti.fr/render_pdf.php?p=1002286;13;Grenoble;45.188530;5.724525
1190;Revue des Nouvelles Technologies de l'Information;EGC;2017;Analyse exploratoire de corpus textuels pour le journalisme d'investigation;Nous proposons un outil de visualisation analytique conçu pour etavec une journaliste d'investigation pour l'exploration de corpus textuels. Notreoutil combine une technique de biclustering disjoint pour extraire des sujets dehaut niveau, avec une méthode de biclustering non-disjoint pour révéler plus fi-nement les variantes de sujets. Une vue d'ensemble des sujets de haut niveau estproposée sous forme d'une treemap, puis une visualisation hiérarchique radialecoordonnée avec une heatmap permet d'inspecter et de comparer les variantesde sujet et d'accéder aux contenus d'origine à la demande.;Nicolas Médoc, Mohammad Ghoniem, Mohamed Nadif;http://editions-rnti.fr/render_pdf.php?p1&p=1002328;http://editions-rnti.fr/render_pdf.php?p=1002328;11;Grenoble;45.188531;5.724526
1191;Revue des Nouvelles Technologies de l'Information;EGC;2017;Anonymiser des données multidimensionnelles à l'aide du coclustering;Dans cet article, nous proposons une méthodologie pour anonymiserune table de données multidimensionnelles contenant des données individuelles(soit n individus décrits par m variables). L'objectif est de publier une table ano-nyme construite à partir d'une table initiale qui protège contre le risque de ré-identification. En d'autres termes, on ne doit pas pouvoir retrouver dans les don-nées publiées un individu présent dans la table originale. La solution proposéeconsite à agréger les données à l'aide d'une technique de coclustering, puis à uti-liser le modèle produit pour générer une table de données synthétiques du mêmeformat que les données initiales. Les données synthétiques, qui contiennent desindividus fictifs, peuvent maintenant être publiées. Les données produites sontévaluées en termes d'utilité pour différentes tâches de fouille (analyse explora-toire, classification) et de niveau de protection.;Françoise Fessant, Tarek Benkhelif, Fabrice Clérot;http://editions-rnti.fr/render_pdf.php?p1&p=1002277;http://editions-rnti.fr/render_pdf.php?p=1002277;8;Grenoble;45.188532;5.724527
1192;Revue des Nouvelles Technologies de l'Information;EGC;2017;Application du coclustering à l'analyse exploratoire d'une table de données;La classification croisée est une technique d'analyse non superviséequi permet d'extraire la structure sous-jacente existante entre les individus et lesvariables d'une table de données sous forme de blocs homogènes. Cette tech-nique se limitant aux variables de même nature, soit numériques soit catégo-rielles, nous proposons de l'étendre en proposant une méthodologie en deuxétapes. Lors de la première étape, toutes les variables sont binarisées selon unnombre de parties choisi par l'analyste, par discrétisation en fréquences égalesdans le cas numérique ou en gardant les valeurs les plus fréquentes dans le cascatégoriel. La deuxième étape consiste à utiliser une méthode de coclusteringentre individus et variables binaires, conduisant à des regroupements d'indivi-dus d'une part, et de parties de variables d'autre part. Nous appliquons cetteméthodologie sur plusieurs jeux de donnée en la comparant aux résultats d'uneanalyse par correspondances multiples ACM, appliquée aux même données bi-narisées.;Aichetou Bouchareb, Marc Boullé, Fabrice Clérot, Fabrice Rossi;http://editions-rnti.fr/render_pdf.php?p1&p=1002279;http://editions-rnti.fr/render_pdf.php?p=1002279;6;Grenoble;45.188533;5.724528
1193;Revue des Nouvelles Technologies de l'Information;EGC;2017;Application mobile pour l'évaluation d'un algorithme de calcul de distance entre des items musicaux;Les systèmes de recommandation permettent de présenter à un utilisa-teur des éléments susceptibles de l'intéresser. La mise en place de tels systèmesdans les domaines culturels soulève souvent le questionnement de la place de ladiversité, de la nouveauté, et surtout de la découverte. Nous pensons que l'êtrehumain, bien qu'ayant ordinairement une tendance à se placer dans une zonede confort correspondant à ce qu'il connaît, apprécie occasionnellement d'êtrepoussé à des explorations le faisant sortir de sa routine. Nous avons développédans cette optique une méthode, basée sur la dissimilarité, qui élargit les centresd'intérêt des utilisateurs. Nous avons réussi à délimiter une zone intermédiaireentre des items « trop similaires » et des items « trop différents ». Afin de vali-der cette hypothèse, nous avons développé une application qui permet de testeret de valider cette méthode. Dans cet article de démonstration, nous expliquonsle concept de « zone intermédiaire », nous détaillons le fonctionnement de l'ap-plication, puis nous présentons les résultats obtenus à partir des tests effectués.;Pierre-René Lhérisson, Fabrice Muhlenbach, Pierre Maret;http://editions-rnti.fr/render_pdf.php?p1&p=1002324;http://editions-rnti.fr/render_pdf.php?p=1002324;11;Grenoble;45.188534;5.724529
1194;Revue des Nouvelles Technologies de l'Information;EGC;2017;Apprentissage d'espaces prétopologiques dans un cadre multi-instance pour la structuration de données;Nous présentons dans cet article une méthode supervisée de structu-ration (en DAG) d'un ensemble d'éléments. Étant donnés une structure cible etun ensemble de relations sur ces éléments, il s'agit d'apprendre un modèle destructuration par combinaison des relations initiales. Nous formalisons ce pro-blème dans le cadre de la théorie de la prétopologie qui permet d'atteindre desmodèles de structuration complexes.Nous montrons que la non-idempotence de la fonction d'adhérence rentre dansle cadre du formalisme de l'apprentissage (supervisé) multi-instance et nous pro-posons un algorithme d'apprentissage reposant sur le dénombrement des «sacs»positifs et négatifs plutôt que sur un ensemble d'apprentissage standard.Une première expérimentation de cette méthode est présentée dans un cadreapplicatif de fouille de textes, consistant à apprendre un modèle de structurationtaxonomique d'un ensemble de termes.;Gaëtan Caillaut, Guillaume Cleuziou;http://editions-rnti.fr/render_pdf.php?p1&p=1002300;http://editions-rnti.fr/render_pdf.php?p=1002300;2;Grenoble;45.188535;5.724530
1195;Revue des Nouvelles Technologies de l'Information;EGC;2017;Apprentissage de structures séquentielles pour l'extraction d'entités et de relations dans des textes d'appels d'offres;Dans cet article nous présentons une étude exploitant des méthodesd'apprentissage automatique de structures séquentielles pour extraire des rela-tions sémantiques dans des textes issus de bases d'appels d'offres. L'une desrelations que nous considérons concerne l'emprise d'un projet d'aménagement,caractérisée par une association entre les concepts qui définissent les infrastruc-tures (bâtiments) et les concepts qui définissent leur(s) surface(s) d'implantation.L'étude propose une analyse comparée d'approches à base de champs condi-tionnels aléatoires (CRF), de CRF d'ordre supérieur (H-CRF), de CRF semi-Markoviens, Modèles de Markov cachés (HMM) et de perceptrons structurés.;Oussama Ahmia, Nicolas Béchet, Pierre-François Marteau;http://editions-rnti.fr/render_pdf.php?p1&p=1002297;http://editions-rnti.fr/render_pdf.php?p=1002297;8;Grenoble;45.188536;5.724531
1196;Revue des Nouvelles Technologies de l'Information;EGC;2017;Approche préventive pour une gestion élastique du traitement parallèle et distribué de flux de données;Dans un contexte de traitement de flux de données, il est importantde garantir à l'utilisateur des propriétés de performance, qualité des résultats etpassage à l'échelle. Mettre en adéquation ressources et besoins, pour n'allouerque les ressources nécessaires au traitement efficace des flux, est un défi d'actualitémajeur au croisement des problématiques du Big Data et du Green IT.L'approche que nous suggérons permet d'adapter dynamiquement et automatiquementle degré de parallélisme des différents opérateurs composant une requêtecontinue selon l'évolution du débit des flux traités. Nous proposons i) unemétrique permettant d'estimer l'activité future des opérateurs selon l'évolutiondes flux en entrée, ii) l'approche AUTOSCALE évaluant a priori l'intérêt d'unemodification du degré de parallélisme des opérateurs en prenant en compte l'impactsur le traitement des données dans sa globalité iii) grâce à une intégrationde notre proposition à Apache Storm, nous exposons des tests de performancecomparant notre approche par rapport à la solution native de cet outil.;Roland Kotto-Kombi, Nicolas Lumineau, Phillipe Lamarre;http://editions-rnti.fr/render_pdf.php?p1&p=1002269;http://editions-rnti.fr/render_pdf.php?p=1002269;1;Grenoble;45.188537;5.724532
1197;Revue des Nouvelles Technologies de l'Information;EGC;2017;Cadre d'Evaluation pour la Méta Analyse de Données;;William Raynaut, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau;http://editions-rnti.fr/render_pdf.php?p1&p=1002314;http://editions-rnti.fr/render_pdf.php?p=1002314;11;Grenoble;45.188538;5.724533
1198;Revue des Nouvelles Technologies de l'Information;EGC;2017;Classification ascendante hiérarchique à noyaux et une application aux données textuelles;"La formule de Lance etWilliams permet d'unifier plusieurs méthodesde classification ascendante hiérarchique (CAH). Dans cet article, nous suppo-sons que les données sont représentées dans un espace euclidien et nous établis-sons une nouvelle expression de cette formule en utilisant les similarités cosinusau lieu des distances euclidiennes au carré. Notre approche présente les avan-tages suivants. D'une part, elle permet d'étendre naturellement les méthodesclassiques de CAH aux fonctions noyau. D'autre part, elle permet d'appliquerdes méthodes d'écrêtage permettant de rendre la matrice de similarités creuseafin d'améliorer la complexité de la CAH. L'application de notre approche surdes tâches de classification automatique de données textuelles montre d'une part,que le passage à l'échelle est amélioré en mémoire et en temps de traitement;d'autre part, que la qualité des résultats est préservée voire améliorée.";Julien Ah-Pine, Xinyu Wang;http://editions-rnti.fr/render_pdf.php?p1&p=1002306;http://editions-rnti.fr/render_pdf.php?p=1002306;8;Grenoble;45.188539;5.724534
1199;Revue des Nouvelles Technologies de l'Information;EGC;2017;Classification d'objets 3D par extraction aléatoire de sous-parties discriminantes pour l'étude du sous-sol en prospection pétrolière;Dans cet article, nous proposons une nouvelle approche de classifi-cation d'objets 3D inspirée des Time Series Shapelets de Ye et Keogh (2009).L'idée est d'utiliser des sous-surfaces discriminantes pour la classification concer-née afin de prendre en compte la nature locale des éléments pertinents. Celapermet à l'utilisateur d'avoir connaissance des sous-parties qui ont été utilespour déterminer l'appartenance d'un objet à une classe. Les résultats obtenusconfirment l'intérêt de la sélection aléatoire de caractéristiques candidates pourla pré-sélection d'attributs en classification supervisée.;François Meunier, Christophe Marsala, Laurent Castanié, Bruno Conche;http://editions-rnti.fr/render_pdf.php?p1&p=1002283;http://editions-rnti.fr/render_pdf.php?p=1002283;8;Grenoble;45.188540;5.724535
1200;Revue des Nouvelles Technologies de l'Information;EGC;2017;Classification multi-labels graduée: Apprendre les relations entre les labels ou limiter la propagation d'erreur ?;La classification multi-labels graduée est la tâche d'affecter àchaque donnée l'ensemble des labels qui lui correspondent selon une échellegraduelle de degrés d'appartenance. Les labels peuvent donc avoir à la foisdes relations d'ordre et de co-occurrence.D'un côté, le fait d'ignorer les relations entre les labels risque d'aboutirà des prédictions incohérentes, et d'un autre côté, le fait de prendre encompte ces relations risque de propager l'erreur de prédiction d'un labelà tous les labels qui lui sont reliés.Les approches de l'état d'art permettent soit d'ignorer les relations entreles labels, soit d'apprendre uniquement les relations correspondant à unestructure de dépendance figée. L'approche que nous proposons permetl'apprentissage des relations entre les labels sans fixer une structure dedépendance au préalable. Elle est basée sur un ensemble de classifieursmono-labels, un pour chaque label. L'idée est d'apprendre d'abord toutesles relations entre les labels y compris les relations cycliques. Ensuite lesdépendances cycliques sont résolues en supprimant les relations d'intérêtminimal. Des mesures sont proposées pour évaluer l'intérêt d'apprendrechaque relation. Ces mesures permettent d'agir sur le compromis entrel'apprentissage de relations pour une prédiction cohérente et la minimisa-tion du risque de la propagation d'erreur de prédiction.;Khalil Laghmari, Christophe Marsala, Mohammed Ramdani;http://editions-rnti.fr/render_pdf.php?p1&p=1002302;http://editions-rnti.fr/render_pdf.php?p=1002302;13;Grenoble;45.188541;5.724536
1201;Revue des Nouvelles Technologies de l'Information;EGC;2017;Classification parcimonieuse pour l'aide à la reconnaissance de cibles radar;Dans le présent papier, nous proposons l'étude et l'application d'unenouvelle approche pour l'aide à la reconnaissance automatique de cibles (ATR,pour Automatic Target Recognition) à partir des images à synthèse d'ouvertureinverse (ISAR, pour Inverse Synthetic Aperture Radar). Cette approche est com-posée de deux phases principales. Dans la première phase, nous utilisons deuxméthodes statistiques pour extraire les caractéristiques discriminants à partir desimages ISAR. Nous nous intéressons dans ce travail aux deux descripteurs multi-échelles issus des deux méthodes SIFT (Scale-Invariant Feature Transform) etla décomposition en ondelettes complexes DT-CWT (Dual-Tree Complex Wa-velet Transform) qui sont calculées disjointement. Ensuite, nous modélisons sé-parément les descripteurs issus des deux méthodes précédentes (SIFT et DT-CWT) par la loi Gamma. Les paramètres statistiques estimés sont utilisés pourla deuxième phase dédiée à la classification. Dans cette deuxième phase, uneclassification parcimonieuse (SRC, pour Sparse Representation-based Classifi-cation) est proposée. Afin d'évaluer et valider notre approche, nous avons eurecours aux données réelles d'images issues d'une chambre anéchoïque. Les ré-sultats expérimentaux montrent que l'approche proposée peut atteindre un tauxde reconnaissance élevé et dépasse largement l'utilisation du même descripteuravec le classifieur machine à vecteurs de support (SVM, pour Support VectorMachine).;Ayoub Karine, Abdelmalek Toumi, Ali Khenchaf, Mohammed El Hassouni;http://editions-rnti.fr/render_pdf.php?p1&p=1002305;http://editions-rnti.fr/render_pdf.php?p=1002305;2;Grenoble;45.188542;5.724537
1202;Revue des Nouvelles Technologies de l'Information;EGC;2017;Co-clustering de données mixtes à base des modèles de mélange;La classification croisée (co-clustering) est une technique non super-visée qui permet d'extraire la structure sous-jacente existante entre les lignes etles colonnes d'une table de données sous forme de blocs. Plusieurs approchesont été étudiées et ont démontré leur capacité à extraire ce type de structure dansune table de données continues, binaires ou de contingence. Cependant, peu detravaux ont traité le co-clustering des tables de données mixtes. Dans cet article,nous étendons l'utilisation du co-clustering par modèles à blocs latents au casdes données mixtes (variables continues et variables binaires). Nous évaluonsl'efficacité de cette extension sur des données simulées et nous discutons seslimites potentielles.;Aichetou Bouchareb, Marc Boullé, Fabrice Rossi;http://editions-rnti.fr/render_pdf.php?p1&p=1002276;http://editions-rnti.fr/render_pdf.php?p=1002276;2;Grenoble;45.188543;5.724538
1203;Revue des Nouvelles Technologies de l'Information;EGC;2017;Comparaison et Évaluation de Mesures de Similarité entre Concepts d'un Treillis;Cet article se situe dans le cadre de l'analyse de concepts formels(ACF) qui fournit des classes (les extensions) d'objets partageant des carac-tères similaires (les intensions), une description par des attributs étant associéeà chaque classe. Dans un article récent, une nouvelle mesure de similarité entredeux concepts dans un treillis de concepts a été introduite, permettant une nor-malisation par la taille du treillis. Dans cet article, nous comparons cette mesurede similarité avec des mesures existantes, soit basées sur la cardinalité des en-sembles ou issues de la conception d'ontologies et basées sur la structure hiérar-chique du treillis. Une comparaison statistique avec des méthodes existantes esteffectuée et testée pour leur consistance.;Florent Domenach, George Portides;http://editions-rnti.fr/render_pdf.php?p1&p=1002289;http://editions-rnti.fr/render_pdf.php?p=1002289;2;Grenoble;45.188544;5.724539
1204;Revue des Nouvelles Technologies de l'Information;EGC;2017;Conception d'un modèle généraliste pour l'évaluation d'un test A/B;;Emmanuelle Claeys, Pierre Gançarski, Myriam Maumy-Bertrand, Hubert Wassner;http://editions-rnti.fr/render_pdf.php?p1&p=1002309;http://editions-rnti.fr/render_pdf.php?p=1002309;11;Grenoble;45.188545;5.724540
1205;Revue des Nouvelles Technologies de l'Information;EGC;2017;Découverte de sous-groupes avec les arbres de recherche de Monte Carlo;Découvrir des règles qui distinguent clairement une classe d'une autrereste un problème difficile. De tels motifs permettent de suggérer des hypothèsespouvant expliquer une classe. La découverte de sous-groupes (Subgroup Disco-very, SD), un cadre qui définit formellement cette tâche d'extraction de motifs,est toujours confrontée à deux problèmes majeurs: (i) définir des mesures dequalité appropriées qui caractérisent la singularité d'un motif et (ii) choisir uneheuristique d'exploration de l'espace de recherche correcte lorsqu'une énuméra-tion complète est irréalisable. À ce jour, les algorithmes de SD les plus efficacessont basés sur une recherche en faisceau (Beam Search, BS). La collection demotifs extraits manque cependant de diversité en raison de la nature gloutonne del'exploration. Nous proposons ici d'utiliser une technique d'exploration récente,la recherche arborescente de Monte Carlo (Monte Carlo Tree Search, MCTS).Le compromis entre l'exploitation et l'exploration ainsi que la puissance de larecherche aléatoire permettent d'obtenir une solution disponible à tout momentet de surpasser généralement les approches de type BS. Notre étude empirique,avec plusieurs mesures de qualité, sur divers jeux de données de référence et dumonde réel démontre la qualité de notre approche.;Guillaume Bosc, Jean-François Boulicaut, Chedy Raïssi, Mehdi Kaytoue;http://editions-rnti.fr/render_pdf.php?p1&p=1002287;http://editions-rnti.fr/render_pdf.php?p=1002287;3;Grenoble;45.188546;5.724541
1206;Revue des Nouvelles Technologies de l'Information;EGC;2017;Deep Dive on Smart Cities by Scaling Reasoning and Interpreting the Semantics of IoT;Modern cities are facing tremendous amount of information, captured from internal in-frastructures and/or exogenous sensors, humanincluded. This talk presents how big and het-erogenous city data has been captured, represented, unified to serve one of the most pressingcity objective: improving quality of city, in particular how understanding and reducing traf-fic congestion. We will also present lessons learnt from the deployment of our system andexperimentation in Dublin (Ireland), Bologna (Italy), Miami (USA) and Rio (Brazil).;Freddy Lécué;http://editions-rnti.fr/render_pdf.php?p1&p=1002264;http://editions-rnti.fr/render_pdf.php?p=1002264;0;Grenoble;45.188547;5.724542
1207;Revue des Nouvelles Technologies de l'Information;EGC;2017;Défi EGC 2017: Modélisation Cost-Sensitive et Enrichissement de données;La conférence EGC'2017 propose un défi dont le contexte est la gestiondes espaces verts pour la ville de Grenoble, et notamment des arbres qui ysont présents. L'objectif est de proposer un modèle basé sur des données fourniesqui permettrait de prédire au mieux les arbres malades, ainsi que la localisationpotentielle de la maladie. Après avoir obtenu quelques résultats intéressantsavec des modèles standards, notre approche utilisant un modèle Cost-SensitiveOne Against All (CSOAA) nous permet d'obtenir une exactitude de 0,86, uneprécision de 0,88, et un rappel de 0,91 sur la prédiction unilabel, et une précision/rappel micro de 0,82/0,74 ainsi qu'une précision/rappel macro de 0,66/0,46pour la prédiction multilabel. L'extraction de connaissances pour la tâche 2 nousa permis de mettre en relief l'intérêt de l'ajout de données sur la nature des maladieset la concentration de la pollution dans la ville.;Vincent Levorato, Michel Lutz, Matthieu Lagacherie;http://editions-rnti.fr/render_pdf.php?p1&p=1002268;http://editions-rnti.fr/render_pdf.php?p=1002268;9;Grenoble;45.188548;5.724543
1208;Revue des Nouvelles Technologies de l'Information;EGC;2017;Description interactive de l'intérêt de l'utilisateur via l'échantillonnage de motifs;La plupart des méthodes d'extraction de motifs requièrent que l'uti-lisateur formalise son intérêt avec une mesure d'intérêt et des seuils. L'utili-sateur est souvent incapable d'expliciter son intérêt mais il saura juger si unmotif donné est pertinent ou non. Dans cet article, nous proposons une nou-velle méthode de découverte de motifs interactive en supposant que seule unepartie des données est intéressante pour l'utilisateur. En intégrant le retour utili-sateur de motifs proposés un à un, notre méthode vise à échantillonner des mo-tifs avec une probabilité proportionnelle à leur fréquence d'apparition au seindes transactions implicitement préférées par l'utilisateur. Nous démontrons quenotre méthode identifie exactement les transactions implicitement préférées parl'utilisateur sous réserve de la consistance de ses retours. Des expérimentationsmontrent les bonnes performances de l'approche en terme de précision et rappel.;Moditha Hewasinghage, Suela Isaj, Arnaud Giacometti, Arnaud Soulet;http://editions-rnti.fr/render_pdf.php?p1&p=1002332;http://editions-rnti.fr/render_pdf.php?p=1002332;13;Grenoble;45.188549;5.724544
1209;Revue des Nouvelles Technologies de l'Information;EGC;2017;Détection de fausses informations dans les réseaux sociaux : vers des approches multi-modales;;Cédric Maigrot, Vincent Claveau, Ewa Kijak;http://editions-rnti.fr/render_pdf.php?p1&p=1002316;http://editions-rnti.fr/render_pdf.php?p=1002316;11;Grenoble;45.188550;5.724545
1210;Revue des Nouvelles Technologies de l'Information;EGC;2017;Enhanced user-user collaborative filtering recommendation algorithm based on semantic ratings;;Wen Zhang, Raja Chiky, Manuel Pozo;http://editions-rnti.fr/render_pdf.php?p1&p=1002318;http://editions-rnti.fr/render_pdf.php?p=1002318;11;Grenoble;45.188551;5.724546
1211;Revue des Nouvelles Technologies de l'Information;EGC;2017;Evolution temporelle de communautés représentatives : mesures et visualisation;La problématique de ce papier est d'identifier dans un graphe dyna-mique les communautés les plus représentatives sur une période donnée, de me-surer leur stabilité, et d'en visualiser les évolutions majeures. Notre cas d'usageconcerne l'étude de la visibilité médiatique des communautés et des individusgrâce aux données relatives aux émissions télévisuelles et radiophoniques entre2011 et 2015. A partir d'une détection de communautés sur l'intégralité de lapériode, nous proposons des mesures de stabilité et d'activité des communautéset proposons une visualisation de leur évolution temporelle.;Haolin Ren, Marie-Luce Viaud, Guy Melançon;http://editions-rnti.fr/render_pdf.php?p1&p=1002308;http://editions-rnti.fr/render_pdf.php?p=1002308;6;Grenoble;45.188552;5.724547
1212;Revue des Nouvelles Technologies de l'Information;EGC;2017;Expression des connaissances en langage naturel : singularité et normalité d'une sélection;;Jérémy Vizzini, Cyril Labbé, François Portet;http://editions-rnti.fr/render_pdf.php?p1&p=1002319;http://editions-rnti.fr/render_pdf.php?p=1002319;11;Grenoble;45.188553;5.724548
1213;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction automatique de paysages en imagerie satellitaire et enrichissement sémantique;Nous présentons ici une méthode originale pour l'automatisation dela détection de paysages dans une image satellite. Deux enjeux majeurs ap-paraissent dans ce processus. Le premier réside dans la faculté à prendre encompte l'ensemble des connaissances expertes tout au long du travail d'analysede l'image. Le second est de réussir à structurer et pérenniser ces connaissancesde façon à les rendre interopérables et exploitables dans le cadre du web de don-nées. Nous présentons en quoi la collaboration de plusieurs stratégies alliant lestraitements de l'image, le calcul de caractéristiques spécifiques et la program-mation logique inductive (PLI), vient alimenter le processus d'automatisation,et comment l'intégration de la connaissance, au travers de la construction d'on-tologies dédiées, permet de répondre pleinement à ces enjeux.;Anne Toulet, Emmanuel Roux, Anne-Elisabeth Laques, Eric Delaître, Laurent Demagistri, Isabelle Mougenot;http://editions-rnti.fr/render_pdf.php?p1&p=1002285;http://editions-rnti.fr/render_pdf.php?p=1002285;8;Grenoble;45.188554;5.724549
1214;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction de chroniques discriminantes;L'extraction de motifs séquentiels vise à extraire des comportementsrécurrents dans un ensemble de séquences. Lorsque ces séquences sont étique-tées, l'extraction de motifs discriminants engendre des motifs caractéristiquesde chaque classe de séquences. Cet article s'intéresse à l'extraction des chro-niques discriminantes où une chronique est un type de motif temporel représen-tant des durées inter-évènements quantitatives. L'article présente l'algorithmeDCM dont l'originalité réside dans l'utilisation de méthodes d'apprentissageautomatique pour extraire les intervalles temporels. Les performances compu-tationnelles et le pouvoir discriminant des chroniques extraites sont évalués surdes données synthétiques et réelles.;Yann Dauxais, David Gross-Amblard, Thomas Guyet, André Happe;http://editions-rnti.fr/render_pdf.php?p1&p=1002278;http://editions-rnti.fr/render_pdf.php?p=1002278;8;Grenoble;45.188555;5.724550
1215;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction de relations pour le peuplement d'une base de connaissance à partir de tweets;Dans une base de connaissance, les entités se veulent pérennes maiscertains événements induisent que les relations entre ces entités sont instables.C'est notamment le cas pour des relations entre organisations, produits, ou marques,entités qui peuvent être rachetées. Dans cet article, nous proposons une approchepermettant d'extraire des relations d'appartenance entre deux entités afin de peu-pler une base de connaissance. L'extraction des relations à partir d'une sourcedynamique d'informations telle que Twitter permet d'atteindre cet objectif entemps réel. L'approche consiste à modéliser les événements en s'appuyant surune ressource lexico-sémantique. Une fois les entités liées au Web des donnéesouvertes (en particulier DBpedia), des règles linguistiques sont appliquées pourfinalement générer les triplets RDF qui représentent les événements.;Cédric Lopez, Elena Cabrio, Frédérique Segond;http://editions-rnti.fr/render_pdf.php?p1&p=1002301;http://editions-rnti.fr/render_pdf.php?p=1002301;8;Grenoble;45.188556;5.724551
1216;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction des évolutions récurrentes dans un unique graphe dynamique attribué;Un grand nombre d'applications nécessitent d'analyser un unique grapheattribué évoluant dans le temps. Cette tâche est particulièrement complexe car lastructure du graphe et les attributs associés à chacun de ses noeuds ne sont pasfigés. Dans ce travail, nous nous focalisons sur la découverte de motifs récurrentsdans un tel graphe. Ces motifs, des séquences de sous-graphes connexes, représententles évolutions récurrentes de sous-ensembles de noeuds et de leurs attributs.Différentes contraintes ont été définies (e.g. fréquence, volume, connectivité,non redondance, continuité) et un algorithme original a été proposé. Lesexpérimentations réalisées sur des jeux de données synthétiques et réelles démontrentl'intérêt de l'approche proposée et son passage à l'échelle.;Zhi Cheng, Frédéric Flouvat, Nazha Selmaoui-Folcher;http://editions-rnti.fr/render_pdf.php?p1&p=1002273;http://editions-rnti.fr/render_pdf.php?p=1002273;8;Grenoble;45.188557;5.724552
1217;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction et chaînage supervisés de connaissances d'un corpus d'entretiens en histoire des sciences;;Benjamin Hervy, Matthieu Quantin, Pierre Teissier;http://editions-rnti.fr/render_pdf.php?p1&p=1002311;http://editions-rnti.fr/render_pdf.php?p=1002311;11;Grenoble;45.188558;5.724553
1218;Revue des Nouvelles Technologies de l'Information;EGC;2017;Extraction et Inférence de Connaissances à partir d'Assemblages Mécaniques Définis par une Représentation CAO 3D;L'extraction de connaissances à partir de modèles géométriques 3Det les raisonnements associés constituent un enjeu important pour permettre ledéveloppement d'ontologies capables de décrire fonctionnellement des produitsmanufacturés. Dans ce contexte, nous nous appuyons sur la logique déductiveapportée par une base de connaissances étroitement couplée à un modeleur géométrique3D. Les raisonnements faisant appel au concept de forme 3D restentdifficiles à formaliser et les informations géométriques difficiles à extraire. Nousproposons une formalisation de propriétés telles que 'à la même forme que','est de la même famille que' pour montrer comment l'extraction d'informationsgéométriques 3D est reliée à ces propriétés. Par la suite, une formalisation depropriétés telles que 'est un empilage', 'est un regroupement' est introduite pourmontrer les raisonnements qui contribuent à la structuration d'assemblages 3D.Ces propriétés sont illustrées à l'aide d'un exemple de pompe hydraulique.;Harold Vilmart, Jean-Claude Léon, Federico Ulliana;http://editions-rnti.fr/render_pdf.php?p1&p=1002266;http://editions-rnti.fr/render_pdf.php?p=1002266;11;Grenoble;45.188559;5.724554
1219;Revue des Nouvelles Technologies de l'Information;EGC;2017;Face2Graph: Base de données graphe et visualisation pour l'annotation d'archives vidéos;Nous proposons dans ce travail d'utiliser la flexibilité des modèlesde base de données graphe, et la représentation intuitive du réseau social afinde visuellement explorer, annoter, et vérifier des détections de visages dans unearchive de 15 années de journaux télévisés.;Adrien Dufraux, Benjamin Renoust, Shin’Ichi Satoh;http://editions-rnti.fr/render_pdf.php?p1&p=1002322;http://editions-rnti.fr/render_pdf.php?p=1002322;13;Grenoble;45.188560;5.724555
1220;Revue des Nouvelles Technologies de l'Information;EGC;2017;Faciliter les contributions personnelles pour préserver la mémoire des événements historiques;Un aspect essentiel dans la préservation du patrimoine culturel résidedans la collecte et l'assemblage des témoignages provenant de citoyens ordi-naires. Dans cet article, nous présentons une architecture logicielle facilitant lasaisie et le partage de témoignages concernant la période de la construction eu-ropéenne au Luxembourg. En rédigeant son témoignage, l'utilisateur obtient lesrésultats d'une extraction de connaissances sur le contenu saisi, indiquant no-tamment des entités et informations liées.;Pierrick Bruneau, Olivier Parisot, Thomas Tamisier;http://editions-rnti.fr/render_pdf.php?p1&p=1002290;http://editions-rnti.fr/render_pdf.php?p=1002290;13;Grenoble;45.188561;5.724556
1221;Revue des Nouvelles Technologies de l'Information;EGC;2017;Génération de RDF à partir de sources de données aux formats hétérogènes;Contrairement à ce que promeut le Web des données, les données exposéespar la plupart des organisations sont dans des formats non-RDF tels queCSV, JSON, ou XML. De plus sur le Web des objets, les objets contraints préférerontdes formats binaires tels que EXI ou CBOR aux formats RDF textuels.Dans ce contexte, RDF peut toutefois servir de lingua franca pour l'interopérabilitésémantique, l'intégration de données aux formats hétérogènes, le raisonnement,et le requêtage. Dans ce but, plusieurs outils et formalismes permettentde transformer des documents non-RDF vers RDF, les plus flexibles étant baséssur des langages de transformation ou de correspondance (GRDDL, XSPARQL,R2RML, RML, CSVW, etc.). Cet article définit un nouveau langage, SPARQLGenerate,qui permet de générer du RDF à partir: (i) d'une base de données RDF,et (ii) d'un nombre quelconque de documents aux formats arbitraires. L'originalitéde SPARQL-Generate est qu'il étend SPARQL 1.1, et peut donc (i) êtreappris facilement par les ingénieurs de la connaissance familiers de SPARQL,(ii) être implémenté au dessus de n'importe quel moteur SPARQL existant, (iii)tirer parti des mécanismes d'extension de SPARQL pour prendre en compte defuturs formats.;Maxime Lefrançois, Antoine Zimmermann, Noorani Bakerally;http://editions-rnti.fr/render_pdf.php?p1&p=1002272;http://editions-rnti.fr/render_pdf.php?p=1002272;8;Grenoble;45.188562;5.724557
1222;Revue des Nouvelles Technologies de l'Information;EGC;2017;Gestion de Connaissances en Temps Réel depuis des Flux Massifs de Données et Apprentissage Automatique;L'analyse en temps-réel de données massives envoyées par des cap-teurs a connu ces dernières années un essor important. Du fait de l'hétérogénéitéde ces données, l'application de modèles de machine learning spécialement ca-librés pour des cas d'usages précis a permis d'extraire et d'inférer des infor-mations de très grandes valeurs. Néanmoins, peu de systèmes proposent uneimplémentation distribuée sur un vrai cluster industriel permettant de tirer profitde capacités de calcul décuplées. Nous présentons ici une démonstration de dé-tection d'anomalie sur réseau souterrain d'eau potable en île-de-France réaliséavec notre plateforme, dénotée WAVES.;Badre Belabbess, Jérémy Lhez, Olivier Curé;http://editions-rnti.fr/render_pdf.php?p1&p=1002329;http://editions-rnti.fr/render_pdf.php?p=1002329;11;Grenoble;45.188563;5.724558
1223;Revue des Nouvelles Technologies de l'Information;EGC;2017;Interopérabilité sémantique libérale pour les services et les objets;Le Web des données promeut l'utilisation de RDF comme modèlepour les données structurées sur le Web. Cependant, la majorité des servicesWeb consomment et exposent principalement du CSV, JSON, ou XML, des formatnon-RDF. Il est peu probable que tous ces services se convertissent un jouraux formats RDF existants. Ceci est d'autant plus vrai dans le contexte du Webdes objets, puisque les formats RDF sont pour la plupart textuels alors que lesobjets contraints préféreront des formats binaires tels que EXI ou CBOR. Danscet article, nous proposons une approche pour permettre l'interopérabilité sémantiquede ces services et objets, tout en leur laissant la liberté d'utiliser leursformats préférés. Notre approche s'ancre sur les principes de l'architecture duWeb et ceux du Web des données liées, et repose sur la définition de PrésentationRDF. En supposant qu'une Présentation RDF soit identifiée par une IRI etdéréférençable sur le Web, nous montrons comment, avec différents protocolesdu Web, un client/serveur peut faire comprendre à l'autre partie comment lecontenu d'une message peut être interprété en RDF, ou généré à partir de RDF.Nous nommons ceci la négociation de Présentation RDF. En utilisant ces principes,nous montrons comment les services et objets existants pourraient êtrerendus interopérables à moindre coût sur le Web Sémantique.;Maxime Lefrançois;http://editions-rnti.fr/render_pdf.php?p1&p=1002271;http://editions-rnti.fr/render_pdf.php?p=1002271;Systèmes distribués;Grenoble;45.188564;5.724559
1224;Revue des Nouvelles Technologies de l'Information;EGC;2017;K-Spectral Centroïd pour des données massives;"Nous nous intéressons à la classification non supervisée de séries chro-nologiques. Pour ce faire, nous utilisons l'algorithme K-Spectral Centroïd (K-SC), une variante des K-Means. K-Spectral Centroïd utilise une mesure de dis-similarité entre séries chronologiques, invariante par translation et par change-ment d'échelle. Cet algorithme est coûteux en temps de calcul : lors de la phased'affectation, il nécessite de tester toutes les translations possibles pour identifierla meilleure ; lors de la phase de représentation, le calcul du nouveau barycentrenécessite l'extraction de la plus petite valeur propre d'une matrice. Nous propo-sons dans ce travail trois optimisations de K-SC. L'identification de la meilleuretranslation peut être réalisée efficacement en utilisant la transformée de Fou-rier discrète. Chaque matrice peut être calculée incrémentalement. Le calcul dunouveau barycentre peut s'effectuer à moindre coût grâce à la méthode de lapuissance itérée. Ces trois optimisations fournissent exactement la même classi-fication que K-SC.";Brieuc Conan-Guez, Alain Gély, Lydia Boudjeloud-Assala, Alexandre Blansché;http://editions-rnti.fr/render_pdf.php?p1&p=1002275;http://editions-rnti.fr/render_pdf.php?p=1002275;2;Grenoble;45.188565;5.724560
1225;Revue des Nouvelles Technologies de l'Information;EGC;2017;Machine Learning Based Classification of Android Apps through Text Features;;Mohamed Guendouz, Abdelmalek Amine, Reda Mohamed Hamou;http://editions-rnti.fr/render_pdf.php?p1&p=1002312;http://editions-rnti.fr/render_pdf.php?p=1002312;13;Grenoble;45.188566;5.724561
1226;Revue des Nouvelles Technologies de l'Information;EGC;2017;Machine Learning for the Semantic Web: filling the gaps in Ontology Mining;In the Semantic Web view, ontologies play a key role. They act as shared vocabulariesto be used for semantically annotating Web resources and they allow to perform deductivereasoning for making explicit knowledge that is implicitly contained within them. However,noisy/inconsistent ontological knowledge bases may occur, being the Web a shared and dis-tributed environment, thus making deductive reasoning no more straightforwardly applicable.Machine learning techniques, and specifically inductive learning methods, could be fruitfullyexploited in this case. Additionally, machine learning methods, jointly with standard reason-ing procedure, could be usefully employed for discovering new knowledge from an ontologicalknowledge base, that is not logically derivable. The focus of the talk will be on various ontol-ogy mining problems and on how machine learning methods could be exploited for coping withthem. For ontology mining are meant all those activities that allow to discover hidden knowl-edge from ontological knowledge bases, by possibly using only a sample of data. Specifically,by exploiting the volume of the information within an ontology, machine learning methodscould be of great help for (semi-)automatically enriching and refining existing ontologies, fordetecting concept drift and novelties within ontologies and for discovering hidden knowledgepatterns (also possibly exploiting other sources of information). If on one hand this means toabandon sound and complete reasoning procedures for the advantage of uncertain conclusions,on the other hand this could allow to reason on large scale and to to dial with the intrinsic uncer-tainty characterizing the Web, that, for its nature, could have incomplete and/or contradictoryinformation.;Claudia d’Amato;http://editions-rnti.fr/render_pdf.php?p1&p=1002262;http://editions-rnti.fr/render_pdf.php?p=1002262;0;Grenoble;45.188567;5.724562
1227;Revue des Nouvelles Technologies de l'Information;EGC;2017;Mesure de la confiance dans les systèmes d'information : application aux données de navires;"Ces dernières années, la prolifération rapide des capteurs et des objetscommunicants de tous types a significativement enrichi le contenu des systèmesd'information. Cependant, cela suscite de nouvelles questions quant à la confianceque l'on peut accorder aux informations et aux sources d'informations. Eneffet, ces sources peuvent être leurrées ou sous l'emprise d'un tiers qui falsifieou altère les informations. Cet article propose donc d'aborder la sécurité dessystèmes d'informations sous l'angle de la confiance dans les sources d'informations.En premier lieu, la définition puis l'évaluation de la confiance dans un réseau hétérogènesont introduits. Une modélisation des sources est ensuite proposée. Laconfiance dans ces sources d'informations est abordée au travers de deux caractéristiques: la compétence et la sincérité. L'extraction de la confiance est réaliséevia un ensemble de mesures de ces deux caractéristiques. Une expérience baséesur plusieurs sources simulées à partir d'un jeu de données réelles montrent lapertinence de l'approche; approche qui peut être transposée à d'autres systèmesd'information. Cette étude est appliquée à l'analyse des données de navigationet de positionnement d'un navire.";Benjamin Costé, Cyril Ray, Gouenou Coatrieux;http://editions-rnti.fr/render_pdf.php?p1&p=1002274;http://editions-rnti.fr/render_pdf.php?p=1002274;1;Grenoble;45.188568;5.724563
1228;Revue des Nouvelles Technologies de l'Information;EGC;2017;Mesure de Similarité entre Treillis Basée sur des Correspondances Explicites;Ce document se situe dans le cadre de l'analyse de concepts formels(ACF), une méthode de hiérarchisation algébrique des données basée sur la no-tion d'intension / extension, partageant maximalement attributs et objets. Nousprésentons ici une mesure de similarité basée sur des correspondances entre deuxtreillis de Galois, définie par un modèle expressif utilisant des correspondancesentre objets et entre attributs des deux treillis. Un point clé de notre approcheest que ces correspondances peuvent ne pas être des fonctions, associant un ob-jet (resp. attribut) d'un treillis avec plusieurs objets (resp. attributs) de l'autretreillis.;Florent Domenach;http://editions-rnti.fr/render_pdf.php?p1&p=1002299;http://editions-rnti.fr/render_pdf.php?p=1002299;2;Grenoble;45.188569;5.724564
1229;Revue des Nouvelles Technologies de l'Information;EGC;2017;Nouveau modèle pour un passage à l'échelle de la 0-subsomption;Le test de -subsomption, opération fondamentale en ProgrammationLogique Inductive (PLI) pour tester la validité d'une hypothèse sur les exemples,est particulièrement coûteux. Ainsi, les systèmes d'apprentissage de PLI les plusrécents ne passent pas à l'échelle. Nous proposons donc un nouveau modèle de-subsomption fondé sur un réseau d'acteurs, dans le but de pouvoir décider lasubsomption sur de très grandes clauses.;Hippolyte Léger, Dominique Bouthinon, Mustapha Lebbah, Hanane Azzag;http://editions-rnti.fr/render_pdf.php?p1&p=1002295;http://editions-rnti.fr/render_pdf.php?p=1002295;Systèmes distribués;Grenoble;45.188570;5.724565
1230;Revue des Nouvelles Technologies de l'Information;EGC;2017;Optimisation des performances dans les entrepôts de données NoSQL en colonnes;Le modèle NoSQL orienté colonnes propose un schéma de donnéesflexible et hautement dénormalisé. Dans cet article, nous proposonsune méthode d'implantation d'un entrepôt de données dans un systèmeNoSQL en colonnes. Notre méthode est basée sur une stratégie de regroupementdes attributs issus des tables de faits et de dimensions, sous formede familles de colonnes. Nous utilisons deux algorithmes OEP et k-means.Pour évaluer notre méthode, nous avons effectué plusieurs tests sur lebenchmark TPC-DS au sein du SGBD NoSQL orienté colonnes Hbase,avec une architecture de type MapReduce sur une plateforme Hadoop.;Mohamed Boussahoua, Omar Boussaid;http://editions-rnti.fr/render_pdf.php?p1&p=1002270;http://editions-rnti.fr/render_pdf.php?p=1002270;1;Grenoble;45.188571;5.724566
1231;Revue des Nouvelles Technologies de l'Information;EGC;2017;Pharmacovigilance du Web Social par une approche fondée sur les bases de connaissances du Web Sémantique;;Damien Leprovost, Marie-Christine Jaulent;http://editions-rnti.fr/render_pdf.php?p1&p=1002320;http://editions-rnti.fr/render_pdf.php?p=1002320;11;Grenoble;45.188572;5.724567
1232;Revue des Nouvelles Technologies de l'Information;EGC;2017;PORGY : a Visual Analytics Platform for System Modelling and Analysis Based on Graph Rewriting;PORGY est un environnement interactif utilisé pour la modélisationde systèmes obtenus àpartir de règles de réécriture, pilotés à l'aide de stratégies et basées sur des graphes utilisantdes noeuds à ports. Cette démonstration présente quelques uns des aspects de visualisation ana-lytique proposés par PORGY. Cette dernière facilite la modélisation du système, sa simulationainsi que l'analyse des résultats à différentes échelles.;Bruno Pinaud, Oana Andrei, Maribel Fernández, Hélène Kirchner, Guy Melançon, Jason Vallet;http://editions-rnti.fr/render_pdf.php?p1&p=1002327;http://editions-rnti.fr/render_pdf.php?p=1002327;11;Grenoble;45.188573;5.724568
1233;Revue des Nouvelles Technologies de l'Information;EGC;2017;Prédiction de défauts dans les arbres du parc végétal Grenoblois et préconisations pour les futures plantations;Nous décrivons dans cet article notre réponse au défi EGC 2017. Uneanalyse exploratoire des données a tout d'abord permis de comprendre les distri-butions des différentes variables et de détecter de fortes corrélations. Nous avonsdéfini deux variables supplémentaires à partir des variables du jeu de données.Plusieurs algorithmes de classification supervisée ont été expérimentés pour ré-pondre à la tâche numéro 1 du défi. Les performances ont été évaluées par va-lidation croisée. Cela nous a permis de sélectionner les meilleurs classifieursuni-label et multi-label. Autant sur la tâche uni-label que multi-label, le meilleurclassifieur dépasse les références d'environ 2%. Nous avons également exploréla tâche numéro 2 du défi. D'une part, des règles d'association ont été recher-chées. D'autre part, le jeu de données a été enrichi avec des connaissances tellesque des données climatiques (pluviométrie, température, vent) ou des donnéestaxonomiques dans le domaine de la botanique (famille, ordre, super-ordre). Enoutre, des données géographiques et cartographiques sont exploitées dans unoutil de visualisation d'une partie des données sur les arbres.;Yelen Per, Kevin Dalleau, Malika Smail-Tabbone;http://editions-rnti.fr/render_pdf.php?p1&p=1002284;http://editions-rnti.fr/render_pdf.php?p=1002284;9;Grenoble;45.188574;5.724569
1234;Revue des Nouvelles Technologies de l'Information;EGC;2017;Prédiction du montant levé lors d'une campagne de financement participatif par la méthode des plus proches voisins;Le financement participatif est un mode de financement d'unprojet faisant appel à un grand nombre de personnes, contrairement auxmodes de financement traditionnels. Il a connu une forte croissance avecl'émergence d'Internet et des réseaux sociaux. Cependant plus de 60 %des projets ne sont pas financés, il est donc important de bien préparersa campagne de financement. De plus, en cours de campagne, il est cru-cial d'avoir une estimation rapide de son succès afin de pouvoir réagirrapidement (restructuration, communication) : des outils de prédictionsont alors indispensables. Nous proposons dans cet article une méthodede prédiction du montant final levé lors d'une campagne de financementparticipatif utilisant l'algorithme k-NN : en utilisant l'historique de cam-pagnes passées, nous déterminons celles qui sont les plus similaires à unecampagne en cours. Nous utilisons alors les montants finaux pour faireune estimation. Nous comparons plusieurs mesures de distance pour dé-terminer les plus proches voisins. Nos résultats indiquent que le dernierétat d'une campagne seul est suffisant pour obtenir une bonne prédiction.;Alexandre Blansché, Dylan Da Conceicao, Dylan Koby;http://editions-rnti.fr/render_pdf.php?p1&p=1002303;http://editions-rnti.fr/render_pdf.php?p=1002303;2;Grenoble;45.188575;5.724570
1235;Revue des Nouvelles Technologies de l'Information;EGC;2017;Prévision à court terme des flux de voyageurs du réseau ferré urbain : une approche par les réseaux bayésiens dynamiques;Nous proposons une approche de prévision à court terme des flux devoyageurs du réseau ferré d'Île-de-France basée sur les réseaux bayésiens dy-namiques. La structure du modèle repose sur les relations de causalité entre lesflux adjacents et permet d'intégrer l'offre de transport. En présence de donnéesmanquantes, l'apprentissage est réalisé via l'algorithme espérance-maximisation(EM) structurel. En appliquant notre approche sur une ligne de métro, les résul-tats obtenus sont globalement supérieurs à ceux des autres méthodes testées.;Jérémy Roos, Stéphane Bonnevay, Gérald Gavin;http://editions-rnti.fr/render_pdf.php?p1&p=1002291;http://editions-rnti.fr/render_pdf.php?p=1002291;Prévisions ;Grenoble;45.188576;5.724571
1236;Revue des Nouvelles Technologies de l'Information;EGC;2017;Prototype de clustering exploratoire pour l'aide à la segmentation des clients;Le clustering est une technique largement répandue pour la définitionde profils dans le cadre de l'aide à la gestion de la relation client (CRM). Cepen-dant, les outils classiques sont généralement limités, car ils ne prennent pas encompte la connaissance métier de l'analyste et ne permettent pas l'explorationinteractive des données. Nous décrivons ici un prototype qui permet à un expertmarketing d'explorer interactivement les données pour la recherche de profilsdes clients, mais aussi d'analyser les profils construits à l'aide de différentesvisualisations synthétiques et d'étudier leurs évolutions au cours du temps.;Adnan El Moussawi, Philippe De Guis, Arnaud Giacometti, Nicolas Labroche, Arnaud Soulet;http://editions-rnti.fr/render_pdf.php?p1&p=1002323;http://editions-rnti.fr/render_pdf.php?p=1002323;11;Grenoble;45.188577;5.724572
1237;Revue des Nouvelles Technologies de l'Information;EGC;2017;Recommandations et prédictions de préférences basées sur la combinaison de données sémantiques et de folksonomie;Dans les systèmes de recommandation, l'approche du filtrage sur lecontenu est revenue en force face à celle du filtrage collaboratif grâce à l'arrivéedu paradigme de l'apprentissage profond et des techniques de word embedding.Dans cette même veine, l'avènement des folksonomies et du web sémantique aapporté une meilleure compréhension des profils des utilisateurs et des caracté-ristiques des articles à recommander. Dans cet article, nous nous intéressons audomaine musical et nous introduisons un nouveau calcul de mesure de préfé-rence intégrée dans un système de recommandations basées sur le contenu. Entestant notre approche sur le jeu de données Last.fm, nous montrons que l'utili-sation de termes issus d'une folksonomie associés à des informations issues duweb sémantique permet d'améliorer le processus de recommandation musicale.;Pierre-René Lhérisson, Fabrice Muhlenbach, Pierre Maret;http://editions-rnti.fr/render_pdf.php?p1&p=1002294;http://editions-rnti.fr/render_pdf.php?p=1002294;Prévisions ;Grenoble;45.188578;5.724573
1238;Revue des Nouvelles Technologies de l'Information;EGC;2017;Reconnaissance de sections et d'entités dans les décisions de justice : application des modèles probabilistes HMM et CRF;Une décision de justice est un document textuel rapportant le dénoue-ment d'une affaire judiciaire. Les juristes s'en servent régulièrement commesource d'interprétation de la loi et de compréhension de l'opinion des juges.La masse disponible de décisions exige des solutions automatiques pour aiderles acteurs du droit. Nous proposons d'adresser certains des défis liés à la re-cherche et l'analyse du volume croissant de décisions de justice en France dansun projet plus global. La première phase de ce projet porte sur l'extraction d'in-formation des décisions dans l'objectif de construire une base de connaissancesjurisprudentielles structurant et organisant les décisions. Une telle base facilitel'analyse descriptive et prédictive de corpus de décisions. Cet article présenteune application des modèles probabilistes pour la segmentation des décisions etla reconnaissance d'entités dans leur contenu (lieu, date, participants, règles deloi, ...). Nos tests montrent l'avantage d'approches basées sur les champs aléa-toires conditionnels (CRF) par rapport à des modèles plus simples et rapidesbasés sur les modèles cachés de Markov (HMM). Nous présentons ici les as-pects techniques de la sélection et l'annotation du corpus d'apprentissage, et ladéfinition de descripteurs discriminants. La spécificité des textes est importanteet doit être prise en compte lors de l'application de méthodes d'extraction d'in-formation dans un domaine spécifique.;Gildas Tagny Ngompé, Sébastien Harispe, Guillaume Zambrano, Jacky Montmain, Stéphane Mussard;http://editions-rnti.fr/render_pdf.php?p1&p=1002281;http://editions-rnti.fr/render_pdf.php?p=1002281;8;Grenoble;45.188579;5.724574
1239;Revue des Nouvelles Technologies de l'Information;EGC;2017;Sélection ciblée des descripteurs visuels pour la recherche d'images: une approche basée sur les règles d'association;;Olfa Allani, Nedra Mellouli, Hajer Baazaoui , Herman Akdag;http://editions-rnti.fr/render_pdf.php?p1&p=1002317;http://editions-rnti.fr/render_pdf.php?p=1002317;11;Grenoble;45.188580;5.724575
1240;Revue des Nouvelles Technologies de l'Information;EGC;2017;Sélection et transformation de variables pour la classification Multi-Label par une approche MDL;La classification multi-label est une extension de la classification su-pervisée au cas de plusieurs labels. Elle a connu un regain d'intérêt récent dansla communauté du machine learning de par son utilité dans plusieurs domaines.Comme pour tout problème de machine learning, le besoin de prétraiter les don-nées multi-label est apparu comme une nécessité afin d'améliorer les perfor-mances des classifieurs. Dans cet article, nous introduisons une nouvelle mé-thode permettant de prétraiter des variables descriptives par discrétisation ougroupement de valeur, dans le cas de plusieurs labels à prédire. Le choix dumeilleur prétraitement est posé comme un problème de sélection de modèle, etest résolu au moyen d'une approche bayésienne. Une étude comparative est réa-lisée avec d'autres méthodes de l'état de l'art afin de positionner la nouvelleméthode et de montrer l'intérêt de la sélection de variables pour la classification.;Sènami C Fréjus Ahomagnon, Nicolas Voisine, Marc Boullé;http://editions-rnti.fr/render_pdf.php?p1&p=1002296;http://editions-rnti.fr/render_pdf.php?p=1002296;2;Grenoble;45.188581;5.724576
1241;Revue des Nouvelles Technologies de l'Information;EGC;2017;Subspace Clustering et Visualisation des Flux de Données;Dans ce papier nous proposons une nouvelle approche de subspaceclustering pour les flux de données, permettant à l'utilisateur de suivre visuel-lement le changement dans le comportement du flux. Cette approche détectel'impact des variables sur l'évolution du flux, Tout en visualisant les étapes dusubspace clustering en temps réel. En premier lieu nous appliquons un clusteringsur l'ensemble de variables afin d'identifer les sous-espaces. Ensuite un cluste-ring est appliqué sur les individus dans chaque sous-espace.;Ibrahim Louhi, Lydia Boudjeloud-Assala, Thomas Tamisier;http://editions-rnti.fr/render_pdf.php?p1&p=1002293;http://editions-rnti.fr/render_pdf.php?p=1002293;3;Grenoble;45.188582;5.724577
1242;Revue des Nouvelles Technologies de l'Information;EGC;2017;Suivi de l'évolution de Clusters de Liens dans des Réseaux Sociaux Dynamiques;De nombreuses méthodes ont été proposées pour extraire des clus-ters des réseaux sociaux. Si un travail important est aujourd'hui mené sur laconception de méthodes innovantes capables de rechercher des clusters de na-ture différente, la plupart des approches font l'hypothèse de réseaux statiques.L'une des récentes méthodes concerne notamment la recherche de liens concep-tuels. Il s'agit d'une nouvelle approche de clustering de liens, qui exploite à lafois la structure du réseau et les attributs des noeuds dans le but d'identifier desliens fréquents entre des groupes de noeuds au sein desquels les noeuds par-tagent des attributs communs. Dans ce travail, nous nous intéressons au suivides liens conceptuels dans des réseaux dynamiques, c'est-à-dire des réseaux quiconnaissent des changements structurels importants. Nous cherchons en parti-culier à comprendre comment les liens conceptuels se forment et évoluent aucours du développement du réseau. Pour ce faire, nous proposons un ensemblede mesures qui visent à capturer des comportements caractérisant l'évolutionde ces clusters. Notre approche est ainsi utilisée pour comprendre l'évolutiondes liens conceptuels extraits sur deux réseaux réels : un réseau de co-auteursd'articles scientifiques et un réseau de communications mobiles. Les résultatsobtenus permettent de mettre en lumière des tendances significatives dans l'évo-lution des clusters sur ces deux réseaux.;Erick Stattner, Martine Collard;http://editions-rnti.fr/render_pdf.php?p1&p=1002280;http://editions-rnti.fr/render_pdf.php?p=1002280;7;Grenoble;45.188583;5.724578
1243;Revue des Nouvelles Technologies de l'Information;EGC;2017;Support uniforme de types de données personnalisés dans RDF et SPARQL;"Les littéraux sont les noeuds terminaux du modèle de données RDF, etpermettent d'encoder des données telles que des nombres (""12.5""ˆˆxsd:decimal),des dates (""2017-01-26T23:57:15""ˆˆxsd:dateTime), ou tout autre type d'information(""vert pomme""ˆˆex:couleur). Les moteurs RDF/SPARQL savent tester l'égalité oucomparer les littéraux RDF dont le type de données leur est connu (ce qui estle cas de xsd:decimal et xsd:dateTime). Mais lorsqu'un type de données est inconnud'un moteur RDF/SPARQL (comme ex:couleur), il n'a à priori aucun moyen d'en« découvrir » la sémantique. Dans cet article, nous attaquons ce problème et étu-dions comment permettre: (i) aux éditeurs de données de publier la définition detypes de données personnalisés sur leWeb, et (ii) aux moteurs RDF/SPARQL dedécouvrir à la volée ces types de données personnalisés, et de les utiliser de ma-nière uniforme. Nous discutons de différentes solutions possibles qui tirent partiedes principes du Web des données, et détaillons une solution concrète basée surle déréférencement et le langage JavaScript, suffisemment générique pour êtreutilisée pour des types de données personnalisés arbitrairement complexes.";Maxime Lefrançois, Antoine Zimmermann;http://editions-rnti.fr/render_pdf.php?p1&p=1002292;http://editions-rnti.fr/render_pdf.php?p=1002292;1;Grenoble;45.188584;5.724579
1244;Revue des Nouvelles Technologies de l'Information;EGC;2017;Sur l'évaluation et l'élaboration d'un jeu de données de référence de bonne qualité en télédétection;En analyse d'images de télédétection, les données de référence, ve-nant étiqueter les objets des images, y jouent un rôle crucial mais sont parfois im-précises voire incertaines et en nombre limité. Dans cet article, nous présentonsune méthodologie pour l'amélioration de données de référence pour la télédé-tection en trois étapes : réalignement des données, évaluation via crowdsourcinget création d'un jeu de données de référence de bonne qualité.;Andrés Troya-Galvis, Pierre Gançarski, Isabelle Mougenot, Laure Berti-Equille;http://editions-rnti.fr/render_pdf.php?p1&p=1002304;http://editions-rnti.fr/render_pdf.php?p=1002304;13;Grenoble;45.188585;5.724580
1245;Revue des Nouvelles Technologies de l'Information;EGC;2017;Un critère d'évaluation pour les K-moyennes prédictives;L'algorithme des K-moyennes prédictives est un des algorithmes declustering prédictif visant à décrire et à prédire d'une manière simultanée. Contr-airement à la classification supervisée et au clustering traditionnel, la perfor-mance de ce type d'algorithme est étroitement liée à sa capacité à réaliser unbon compromis entre la description et la prédiction. Or, à notre connaissance,il n'existe pas dans la littérature un critère analytique permettant de mesurer cecompromis. Cet article a pour objectif de proposer une version modifiée de l'in-dice Davies-Bouldin, nommée SDB, permettant ainsi d'évaluer la qualité des ré-sultats issus de l'algorithme des K-moyennes prédictives. Cette modification sebase sur l'intégration d'une nouvelle mesure de dissimilarité permettant d'éta-blir une relation entre la proximité des observations en termes de distance etleur classe d'appartenance. Les résultats expérimentaux montrent que la versionmodifiée de l'indice DB parvient à mesurer la qualité des résultats issus de l'al-gorithme des K-moyennes prédictives.;Oumaima Alaoui Ismaili, Vincent Lemaire, Antoine Cornuéjols;http://editions-rnti.fr/render_pdf.php?p1&p=1002288;http://editions-rnti.fr/render_pdf.php?p=1002288;2;Grenoble;45.188586;5.724581
1246;Revue des Nouvelles Technologies de l'Information;EGC;2017;Un générateur de réseaux dynamiques attribués avec structure communautaire;Nous proposons une nouvelle approche pour générer des graphes dy-namiques avec attributs munis d'une structure communautaire reflétant les pro-priétés connues des graphes de terrain comme l'attachement préférentiel ou l'ho-mophilie. Le générateur développé permet de construire une suite de graphesformant ainsi un réseau dynamique. Il offre la possibilité de visualiser l'évolu-tion de ces graphes à travers une interface dédiée. Cette interface présente aussiplusieurs mesures évaluées sur chacun des graphes du réseau pour vérifier dansquelle mesure les propriétés du réseau sont préservées au cours de son évolution.;Oualid Benyahia, Christine Largeron, Baptiste Jeudy, Osmar R. Zaïane;http://editions-rnti.fr/render_pdf.php?p1&p=1002321;http://editions-rnti.fr/render_pdf.php?p=1002321;11;Grenoble;45.188587;5.724582
1247;Revue des Nouvelles Technologies de l'Information;EGC;2017;Un Modèle de Factorisation de Poisson pour la Recommandation de Points d'Intérêt;L'explosion des volumes de données circulant sur les réseauxsociaux géo-localisés (LBSN) rend possible l'extraction des préférencesdes utilisateurs. En particulier ces préférences peuvent être utilisées pourrecommander à l'utilisateur des points d'intérêt en adéquation avec sonprofil. Aujourd'hui la recommandation de points d'intérêt est devenueune composante essentielle des LBSN. Malheureusement les méthodesde recommandation traditionnelles échouent à s'adapter aux contraintespropres aux LBSN, telles que la ”sparsité” très élevée des données, ouprendre en compte l'influence géographique. Dans ce papier nous pré-sentons un modèle de recommandation basée sur la factorisation de Pois-son qui offre une solution efficace à ces contraintes. Nous avons testénotre modèle via des expérimentations sur un jeu de données réalisteissu du LBSN Foursquare. Ces expériences nous ont permis de démon-trer une meilleure qualité de recommandation que 3 modèles de l'état-de-l'art.;Jean-Benoît Griesner, Talel Abdesssalem, Hubert Naacke;http://editions-rnti.fr/render_pdf.php?p1&p=1002307;http://editions-rnti.fr/render_pdf.php?p=1002307;Prévisions ;Grenoble;45.188588;5.724583
1248;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une Approche d'Extraction de Motifs Graduels (Fermés) Fréquents Sous Contrainte de la Temporalité;La fouille de motifs graduels a pour but la découverte de co-variationsfréquentes entre attributs numériques dans une base de données. Plusieurs algo-rithmes d'extraction automatique de tels motifs ont été proposés. La principaledifférence entre ces algorithmes réside dans la sémantique de variation considé-rée. Dans certains domaines d'application, on trouve des bases de données dontles objets sont munis d'une relation d'ordre temporel. Ainsi, du fait de leur sé-mantique de variation, les algorithmes de la littérature sont inadaptés pour detelles données. Dans ce contexte, nous proposons une approche de fouille demotifs graduels sous contrainte d'ordre temporel, qui réduit le nombre de motifsgénérés. Une étude expérimentale sur des bases de données paléoécologiquespermet d'apprendre les groupements d'indicateurs qui modélisent l'évolution dela biodiversité. Les connaissances apportées par ces groupements montre l'inté-rêt de notre approche pour le domaine environnemental.;Jerry Lonlac, Yannick Miras, Aude Beauger, Marie Pailloux, Jean-Luc Peiry , Engelbert Mephu Nguifo;http://editions-rnti.fr/render_pdf.php?p1&p=1002282;http://editions-rnti.fr/render_pdf.php?p=1002282;8;Grenoble;45.188589;5.724584
1249;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une approche innovante pour la compréhension des comportements de diffusion : personnalité et neutralité;;Didier Henry, Erick Stattner, Martine Collard;http://editions-rnti.fr/render_pdf.php?p1&p=1002315;http://editions-rnti.fr/render_pdf.php?p=1002315;11;Grenoble;45.188590;5.724585
1250;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une approche logique pour la fouille de règles d'association;La découverte de règles d'association à partir de données transaction-nelles est une tâche largement étudiée en fouille de données. Les algorithmesproposés dans ce cadre partagent la même méthodologie en deux étapes à savoirl'énumération des itemsets fréquents suivie par l'étape de génération de règles.Dans cet article, nous proposons une nouvelle approche basée sur la satisfiabilitépropositionnelle pour extraire les règles d'association en une seule étape. Pourmontrer la flexibilité et la déclarativité de notre approche, nous considérons éga-lement deux autres variantes, à savoir la fouille de règles d'association ferméeset la fouille de règles indirectes. Les expérimentation sur plusieurs jeux de don-nées montrent que notre approche offre de meilleures performances comparée àdes approches spécialisées.;Abdelhamid Boudane, Said Jabbour, Lakhdar Sais, Yakoub Salhi;http://editions-rnti.fr/render_pdf.php?p1&p=1002298;http://editions-rnti.fr/render_pdf.php?p=1002298;3;Grenoble;45.188591;5.724586
1251;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une approche sociologique de la place des calculs dans les mondes numériques;Dans cette présentation, on souhaite présenter un regard de sociologue sur les transformationssociales, politiques et culturelles du développement des mondes numériques dans nos sociétés.Les enjeux que doivent relever la fabrication d'environnements informatiques prennentaujourd'hui de plus en plus d'importance : protection de la vie privée, personnalisation descalculs, guidage des conduites, ouverture des données, éthique des automates, etc. Commentnos sociétés réagissent-elles et s'adaptent-elles à ces mutations ? Dans cette cnférence, on proposeune réflexion sur le rôle joué par les algorithmes du web dans la construction de l'espacepublic numérique. Comment les calculateurs produisent-ils de la visibilité ? A partir de quelsprincipes le PageRank de Google, les métriques du web social ou les outils de recommandationdécident-ils de donner la prééminence à telle information plutôt qu'à telle autre ? Cesdifférentes familles de calcul cherchent à mesurer et à valoriser des principes différents : lapopularité, l'autorité, la réputation et la prédiction efficace. L'approche proposée dans cetteconférence soutient que les manières de calculer enferment des représentations particulièresdes individus et de leur place dans nos sociétés. Comprendre les algorithmes c'est aussi unmoyen de redonner du pouvoir aux utilisateurs et de favoriser une critique éclairée de la manièredont le calcul s'introduit de plus en plus dans nos vies numériques.;Dominique Cardon;http://editions-rnti.fr/render_pdf.php?p1&p=1002263;http://editions-rnti.fr/render_pdf.php?p=1002263;0;Grenoble;45.188592;5.724587
1252;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une mesure d'expertise pour le crowdsourcing;Le crowdsourcing, un enjeu économique majeur, est le fait d'externaliserune tâche interne d'une entreprise vers le grand-public, la foule. C'estainsi une forme de sous-traitance digitale destinée à toute personne susceptiblede pouvoir réaliser la tâche demandée généralement rapide et non automatisable.L'évaluation de la qualité du travail des participants est cependant un problèmemajeur en crowdsourcing. En effet, les contributions doivent être contrôlées pourassurer l'efficacité et la pertinence d'une campagne. Plusieurs méthodes ont étéproposées pour évaluer le niveau d'expertise des participants. Ce travail a la particularitéde proposer une méthode de calcul de degrés d'expertise en présencede données dont l'ordre de classement est connu. Les degrés d'expertise sont ensuiteconsidérés sur des données sans ordre pré-établi. Cette méthode fondée surla théorie des fonctions de croyance tient compte des incertitudes des réponseset est évaluée sur des données réelles d'une campagne réalisée en 2016.;Hosna Ouni, Arnaud Martin, Laetitia Gros, Mouloud Kharoune, Zoltan Miklos;http://editions-rnti.fr/render_pdf.php?p1&p=1002267;http://editions-rnti.fr/render_pdf.php?p=1002267;7;Grenoble;45.188593;5.724588
1253;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une métrique de sélection de variables appliquée à la centralité et à la détection des rôles communautaires;La F-Mesure de trait est une métrique de sélection de variables statistiquesans paramètres qui a montré de bonnes performances pour la classification,l'étiquetage de clusters ou encore la mesure de qualité des clusters. Danscet article, nous proposons d'évaluer son utilisation dans le contexte des graphesde terrain et de leur structure communautaire pour bénéficier de son systèmesans paramètres et de ses performances bien évaluées. Nous étudions donc surdes graphes synthétiques réalistes les corrélations qui existent entre la F-Mesurede trait et certaines mesures de centralité, mais surtout avec des mesures destinéesà caractériser le rôle communautaire des noeuds. Nous montrons ainsi quecette mesure est liée à la centralité des noeuds du réseau, et qu'elle est particulièrementadaptée à la mesure de leur connectivité au regard de la structurede communautés. Nous observons par ailleurs que les mesures usuelles de détectiondes rôles communautaires sont fortement dépendantes de la taille descommunautés alors que celles que nous proposons sont par définition liées à ladensité de la communauté, ce qui rend les résultats comparables d'un réseau àun autre. Ceci offre donc la possibilité d'applications comme le suivi temporelde la structure des communautés. Enfin, le processus de sélection appliqué auxnoeuds permet de disposer d'un système universel, contrairement aux seuils fixésauparavant empiriquement pour l'établissement des rôles communautaires.;Nicolas Dugué, Jean-Charles Lamirel;http://editions-rnti.fr/render_pdf.php?p1&p=1002265;http://editions-rnti.fr/render_pdf.php?p=1002265;7;Grenoble;45.188594;5.724589
1254;Revue des Nouvelles Technologies de l'Information;EGC;2017;Une plateforme d'analyse d'opinions en temps réel sur Twitter avec recommandation;;Noureddine Azzouza, Karima Akli-Astouati, Samy Ait-Bachir, Amira Oussalah;http://editions-rnti.fr/render_pdf.php?p1&p=1002313;http://editions-rnti.fr/render_pdf.php?p=1002313;11;Grenoble;45.188595;5.724590
1255;Revue des Nouvelles Technologies de l'Information;EGC;2017;Veille d'Information sur le Web avec Re-Watch;Les algorithmes d'apprentissage automatique peuvent être utilisés pourcréer des outils de recommandation qui permettent de prédire la pertinence d'undocument pour une thématique de veille donnée en se basant sur les précédentsjugements de pertinence donnés pour cette thématique pour d'autres documents.Ces outils de recommandation permettent de filtrer dans un flux entrant de do-cuments ceux qui sont susceptibles d'être pertinents sans que l'utilisateur aitbesoin de déterminer lui-même les mots clefs marquant l'adéquation d'un do-cument pour un sujet de la veille. Bien que cette problématique de rechercheait été abondamment abordée, les outils de veille d'information pour le web in-tégrant un apprentissage en sont encore à leur balbutiements. Nous présentonsici l'application web Re-Watch permettant la définition d'un thème de veille, lasélection de sources d'information sur le web relatives à ce thème et l'adaptationdes scores de pertinence des documents aux retours de l'utilisateur. L'applicationpermet aussi, pour chaque thème, une auto-évaluation de la qualité du filtrage etune interrogation du moteur de recherche Google. Cette application encore encours de développement est néanmoins actuellement fonctionnelle et accessiblesur le web à l'url suivante : http://www.specific search.com.;Christophe Brouard, Christian Pomot;http://editions-rnti.fr/render_pdf.php?p1&p=1002331;http://editions-rnti.fr/render_pdf.php?p=1002331;11;Grenoble;45.188596;5.724591
1256;Revue des Nouvelles Technologies de l'Information;EGC;2017;Vers un échantillonnage de flux de données transformé;;Olivier Parisot, Thomas Tamisier;http://editions-rnti.fr/render_pdf.php?p1&p=1002310;http://editions-rnti.fr/render_pdf.php?p=1002310;11;Grenoble;45.188597;5.724592
1257;Revue des Nouvelles Technologies de l'Information;EGC;2017;Vers une instance française de NELL : chaîne TLN multilingue et modélisation d'ontologie;Nous présentons les étapes de préparation de la création d'une ins-tance nouvelle de NELL dédiée au français. NELL est à la fois un processusde lecture et de compréhension automatique du Web et un ensemble de basede connaissances de faits en anglais, en portuguais et très prochainement enfrançais. Cette mise en place de la nouvelle instance de NELL a donné lieu àl'amélioration de la chaîne NLP en la généralisant au multilangue, ainsi qu'audéveloppement d'une ontologie par correspondance avec l'ontologie en anglais.Nous présenterons le processus de mise en place et de lancement de la nouvelleinstance NELL Français avec l'interface de visualisation et de supervision hu-maine des données collectées.;Maisa Cristina Duarte, Pierre Maret;http://editions-rnti.fr/render_pdf.php?p1&p=1002326;http://editions-rnti.fr/render_pdf.php?p=1002326;11;Grenoble;45.188598;5.724593
1258;Revue des Nouvelles Technologies de l'Information;EGC;2017;VIPE : un outil interactif de classification multilabel de messages courts;Nous présentons un outil interactif de classification multilabel déve-loppé au sein du groupe Orange et utilisé pour l'analyse d'opinions. Basé sur unalgorithme de factorisation rapide de matrice, il permet à un utilisateur d'impor-ter des textes courts (tweets, mails, enquêtes, ...), de définir des labels d'intérêts(« client globalement satisfait », « évoque la rapidité du débit »,...) et de propo-ser pour chaque texte des recommandations de labels et pour chaque label desrecommandations de textes.;Franck Meyer, Sylvie Tricot, Pascale Kuntz, Wissam Siblini;http://editions-rnti.fr/render_pdf.php?p1&p=1002330;http://editions-rnti.fr/render_pdf.php?p=1002330;11;Grenoble;45.188599;5.724594
1259;Revue des Nouvelles Technologies de l'Information;EGC;2017;“Engage moi”: From retrieval effectiveness, user satisfaction to user engagement;"The effective prediction of a click remains a primary challenge in the areas of search, digitalmedia and online advertising. In the context of search, satisfying a userâ&#728;A ´ Zs information needby returning results that they will click on is an important objective in any information retrievalsystem. Consequently, information retrieval systems have had a long and varied history of howto evaluate their effectiveness of responding to a given query. However, building such a systemthat not only only returns relevant results to a user query but also encourages a long-termrelationship between the user and the system is far more challenging. In this talk, we reviewthe current state-of-the-art evaluation approaches for search before exploring other ways ofquantifying more long-term engagement measures. Finally, the talk ends with a proposal ofhow the two approaches can be considered together to create a service that optimises for thequery and the longer term engagement aspects.";Mounia Lalmas;http://editions-rnti.fr/render_pdf.php?p1&p=1002261;http://editions-rnti.fr/render_pdf.php?p=1002261;0;Grenoble;45.188600;5.724595
